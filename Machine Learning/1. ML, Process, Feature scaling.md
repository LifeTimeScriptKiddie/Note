# ML Process
Data pre-processing
	Import
	Clean
	Split into training & test sets
Modeling
	Build
	Train
	Make Predition
Evaluatiion
	Calculate performance metirc 
	Make verdict


# Feature scaling
	Feature scaling is a method used to normalize the range of independent variables or features of data. In machine learning, it is generally a good practice to scale the features so that they have a unit variance.
	
	There are two common ways to perform feature scaling:
	
	1.  Min-Max scaling (Normalization): This method scales the data by transforming the data such that the minimum value becomes 0 and the maximum value becomes 1. This is done by subtracting the minimum value from each data point and dividing the result by the range (i.e., the difference between the maximum and minimum values).
	    
	2.  Standardization: This method scales the data by transforming the data such that the mean becomes 0 and the standard deviation becomes 1. This is done by subtracting the mean value from each data point and dividing the result by the standard deviation.
	    
	
	Feature scaling is important because it can help the model to converge faster and perform better. It is especially important when the features have different scales and units, as is often the case in real-world data. For example, if one feature is in dollars and another feature is in number of transactions, they will have very different scales and units, and this can cause problems when training a machine learning model. Feature scaling can help to mitigate these issues by scaling the data so that all the features are on a similar scale.
	Always applied to column. 

## Normalization
	Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. This is done by subtracting the minimum value from each data point and dividing the result by the range (i.e., the difference between the maximum and minimum values). This technique is useful when the scale of the data is not known or not relevant, and when the data follows a Gaussian (normal) distribution or a uniform distribution.

## Standarlization
	Standardization, on the other hand, is a scaling technique in which values are shifted and rescaled so that they have a mean of 0 and a standard deviation of 1. This is done by subtracting the mean value from each data point and dividing the result by the standard deviation. This technique is useful when the data follows a Gaussian (normal) distribution and when the scale of the data is known or relevant.

In summary, normalization scales the data between 0 and 1, while standardization scales the data so that it has a mean of 0 and a standard deviation of 1. Both techniques are useful for preprocessing data before training a machine learning model.
